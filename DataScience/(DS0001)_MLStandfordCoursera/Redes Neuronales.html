<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css" />
    <title>Redes Neuronales</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!-- For LaTeX -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <!-- Code Highlight -->
    <link
      rel="Stylesheet"
      type="text/css"
      href="https://albamr09.github.io/atom-one-light.min.css"
    />
  </head>
  <body>
    <a
      href="https://albamr09.github.io/"
      style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      "
      >Index</a
    >
    <hr />
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Redes Neuronales"><h1 id="Redes Neuronales" class="header"><a href="#Redes Neuronales">Redes Neuronales</a></h1></div>

<hr />

<ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Estructura">Estructura</a>

<li>
<a href="Redes Neuronales.html#Redes Neuronales-Datos de entrada">Datos de entrada</a>

<li>
<a href="Redes Neuronales.html#Redes Neuronales-Propagación">Propagación</a>

<ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Propagación-Clasificación múltiple">Clasificación Múltiple</a>  

</ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Función de coste">Función de coste</a>

<ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Función de coste-Regularización">Regularización</a>

</ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Retropropagación">Retropropagación</a>

<ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Retropropagación-Derivada de la función de coste">Derivada de la Función de Coste</a>

<li>
<a href="Redes Neuronales.html#Redes Neuronales-Retropropagación-Capas intermedias">Capas Intermedias</a>

<li>
<a href="Redes Neuronales.html#Redes Neuronales-Retropropagación-Ejemplo de retropropagación">Ejemplo de Retropropagación</a>

</ol>
<li>
<a href="Redes Neuronales.html#Redes Neuronales-Algoritmo">Algoritmo</a>

</ol>

<hr />

<div id="Redes Neuronales-Estructura"><h2 id="Estructura" class="header"><a href="#Redes Neuronales-Estructura">Estructura</a></h2></div>

<p>
Dado un ejemplo \(X\), tal que \(X\) es un vector fila \(1 \times 3\):
</p>

\[%align
x = 
\begin{bmatrix}x_1 &amp; x_2 &amp; x_3\end{bmatrix}
\]

<p>
Tenemos que las entradas a la red neuronal con un capa se introducen como sigue:
</p>

<p>
<img src="./assets/nn_ejemplo.svg" alt="Ejemplo Red Neuronal" style="width:350px;height:200px" />
</p>

<p>
De tal manera que obtenemos una salida \(h_\Theta(x)\).
</p>

<p>
Supongamos que utilizamos la función sigmoide (problema de clasificación con dos clases):
</p>

\[%align
h_\Theta(X) = \frac{1}{1+e^{-\Theta^TX}}
\]

<p>
Donde:
</p>


\[%align
\Theta^TX = \sum_{j=0}^m\sum_{i=0}^n \theta_{i} \cdot x_{ij}
\]

<p>
Con \(x_{01} = 1\), para multiplicar con el término independiente de los pesos \(\theta_0\).
</p>

<p>
Normalmente \(\Theta\) está dividido en:
</p>

<ul>
<li>
<span id="Redes Neuronales-Estructura-Término independiente/sesgo"></span><strong id="Término independiente/sesgo">Término independiente/sesgo</strong> (bias): \(\theta_0\), también denotado como \(b\).

<li>
<span id="Redes Neuronales-Estructura-Pesos"></span><strong id="Pesos">Pesos</strong> (bias): \(\theta_i\), con \(1 \leq i \leq n\), también denotados como \(w\).

</ul>

<div id="Redes Neuronales-Datos de entrada"><h2 id="Datos de entrada" class="header"><a href="#Redes Neuronales-Datos de entrada">Datos de entrada</a></h2></div>

<ul>
<li>
\(X = (x_{ij})\) una matriz \(n \times m\) donde cada \(x_{ij}\) es la característica \(i\) del ejemplo \(j\), tal que 
\begin{align}
X = 
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1m} \\
\cdots &amp; \ddots &amp; \cdots \\
x_{n1} &amp; \cdots &amp; x_{nm} \\
\end{bmatrix} 
\end{align}

</ul>

<p>
<em>Cada columna es un ejemplo</em>
</p>

<p>
<em>En cada fila están los valores de una característica</em>
</p>

<p>
Para regresión lineal, donde hay una salida por cada ejemplo \(j\):
</p>

<ul>
<li>
\(Y = (y_j)\) es un vector fila \(1\times m\) donde cada \(y_j\) es la salida real para el ejemplo \(j\), tal que:

<li>

\begin{align}
Y = \begin{bmatrix} 
y_{1} &amp; \cdots &amp; y_{m} \\
\end{bmatrix} 
\end{align}

</ul>

<p>
Para regresión logística, donde hay \(c\) salidas por cada ejemplo \(j\) (cada salida es la probabilidad de que el ejemplo sea de la clase \(l\): <a href="Redes Neuronales.html#Redes Neuronales-Propagación-Clasificación múltiple">Clasificación múltiple</a>).
</p>

<ul>
<li>
\(Y = (y_{ij})\) es una matriz \(c\times m\) donde cada \(y_{ij}\) es la salida real para el ejemplo \(j\) y la clase \(i\), es decir es un valor binario que indica si el ejemplo \(j\) pertenece (1) o no (0) a la clase \(i\), tal que:

</ul>

\begin{align}
Y = \begin{bmatrix} 
y_{11} &amp; \cdots &amp; y_{m1} \\
\vdots &amp; \ddots &amp; \vdots \\
y_{c1} &amp; \cdots &amp; y_{mc} \\
\end{bmatrix} 
\end{align}

<div id="Redes Neuronales-Propagación"><h2 id="Propagación" class="header"><a href="#Redes Neuronales-Propagación">Propagación</a></h2></div>

<div id="Redes Neuronales-Propagación-Notación"><h4 id="Notación" class="header"><a href="#Redes Neuronales-Propagación-Notación">Notación</a></h4></div>

<ul>
<li>
\(k\) es el número de capas de la red

<li>
Para cada capa \(j\), asumimos que la capa tiene \(q\) nodos.

<li>
\(a_i^{(j)}\): Nodo \(i\) en la capa \(j\)

<li>
\(\Theta^{(j)}\): Matriz de pesos de la capa \(j\) a la capa \(j+1\)

<li>
La dimensión de cada \(\Theta^{(j)}\) es \(S_{j} \times (S_{j-1} + 1)\)

<ul>
<li>
\(S_{j}\): número de características/nodos en la capa \(j\).

<li>
\((S_{j-1} + 1)\): número de características en la capa \(j - 1\) más 1 (término independiente en cada capa)

</ul>
</ul>

<div id="Redes Neuronales-Propagación-Funcionamiento"><h4 id="Funcionamiento" class="header"><a href="#Redes Neuronales-Propagación-Funcionamiento">Funcionamiento</a></h4></div>

<p>
Sean \(k\) el número de capas de la red. Para cada capa \(j\), on \(a^0 = X_l\) (donde \(X_l\) es un ejemplo: un vector columna en \(X\)):
</p>

<p>
Primero se calcula el término intermedio \(z^{(j)}\), que es un vector columna \(S_{(j)} \times 1\):
</p>

\begin{align}
z^{(j)} = \Theta^{(j)} \cdot a^{(j-1)}
\end{align}


<p>
Donde, como ya hemos dicho, \(\Theta\) es la matriz \(S_{(j)} \times S_{(j-1)}\) de pesos de la capa \(j\), siendo \(p\) el número de nodos en la capa \((j-1)\) más uno (el término independiente).
</p>

<p>
A continuación se aplica sobre \(z^{(j)}\) la función de activación, tal que:
</p>


\begin{align}
a^{(j)} = g(z^{(j)})
\end{align}


<p>
Esto se representa gráficamente en la siguiente imagen:
</p>

<p>
<img src="./assets/nn_ejemplo_1.svg" alt="Ejemplo Red Neuronal" style="width:1050px;height:600px" />
</p>

<p>
Cabe destacar que la función de activación de la última capa suele ser distinta al resto. De todas formas, las funciones de activación no tienen por qué ser iguales, nosotros en estes ejemplos hemos utilizado la función sigmoide, pero depende de la aplicación que se le quiera da a la red neuronal.
</p>

<div id="Redes Neuronales-Propagación-Clasificación múltiple"><h3 id="Clasificación múltiple" class="header"><a href="#Redes Neuronales-Propagación-Clasificación múltiple">Clasificación múltiple</a></h3></div>

<p>
Para crear una red neuronal que permita trabajar con \(c\) clases lo que hacemos es hacer que la red neuronal tenga \(c\) nodos en su capa de salida. Esto se ilustra en la siguiente imagen:
</p>

<p>
<img src="./assets/nn_ejemplo_multi.svg" alt="Ejemplo Red Neuronal Clasificación Múltiple" style="width:950px;height:550px" />
</p>

<p>
De tal manera que ahora, cada salida \(y_j\) será un vector columna \(c\times1\), donde existe un valor por cada categoría, al igual que la hipótesis para el ejemplo \(j\), \(h_\Theta(x_j)\), es un vector columna \(c\times1\).
</p>

<p>
Como podemos ver, los valores de \(y_j\) indican claramente a qué clase pertenece el ejemplo \(j\) (clase 3), mientras que la hipótesis \(h_\Theta(x_j)\) ofrece, para cada clase (columna) la probabilidad de que el ejemplo \(j\) pertenezca a esa clase.
<hr />
</p>

<div id="Redes Neuronales-Función de coste"><h2 id="Función de coste" class="header"><a href="#Redes Neuronales-Función de coste">Función de coste</a></h2></div>

<div id="Redes Neuronales-Función de coste-Notación"><h4 id="Notación" class="header"><a href="#Redes Neuronales-Función de coste-Notación">Notación</a></h4></div>

<p>
Como ya hemos visto en función del número de clases la salida tendrá distinta forma:
</p>

<ul>
<li>
<span id="Redes Neuronales-Función de coste-Notación-Clasificación binaria"></span><strong id="Clasificación binaria">Clasificación binaria</strong>: para cada ejemplo \(j\), \(y_j \in \{0, 1\}\), \(h_\Theta(x_j) \in \mathbb{R}\)

<li>
<span id="Redes Neuronales-Función de coste-Notación-Clasificación múltiple"></span><strong id="Clasificación múltiple">Clasificación múltiple</strong>: para cada ejemplo \(j\), \(y \in \mathbb{R}^c\), \(h_\Theta(x_j) \in \mathbb{R}^c\), donde \(c\) es el número de clases

<li>
Sea \(k\) el número de capas y \(S_i\) el número de nodos en la capa \(i\).

<li>
Sea \(Y=(y_{ij})\) una matriz \(c\times m\), donde \(m\) es el número de ejemplos y cada \(y_{j}\) es el vector columna \(c\times1\) de salida para el ejemplo \(j\).

</ul>

<hr />

<p>
Definimos la función de coste como sigue:
</p>

\begin{align}
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]\right\}
\end{align}

<p>
El primer sumatorio que va de 1 a \(m\) se encarga de calcular el coste para cada ejemplo \(j\). Mientras que el segundo sumatorio, que va de 1 a \(c\), se encarga de calcular el coste para cada nodo de salida.
</p>

<p>
Esta función se aplica sobre los \(k\) nodos en la capa de salida. 
</p>

<p>
<a href="Ejemplo Cálculo Función de Coste.html">Ejemplo Cálculo Función de Coste</a>
</p>


<div id="Redes Neuronales-Función de coste-Regularización"><h4 id="Regularización" class="header"><a href="#Redes Neuronales-Función de coste-Regularización">Regularización</a></h4></div>

<p>
Definimos la función de coste introduciendo regularización como sigue:
</p>

\begin{align}
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]\right\} + \frac{\lambda}{2m} \sum_{q=1}^k \sum_{i=1}^{S_q}\sum_{j=1}^{S_{q+1}} (\theta_{ji}^{(q)})^2
\end{align}


<p>
Antes de nada, recordar que \(S_q\) denota el número de nodos en la capa \(q\). Entonces, el primer término de la función es igual que cuando no se aplicaba regularización. Expliquemos el segundo término. La regularización, en este caso, consiste en sumar todos los pesos de la red neuronal, por lo tanto:
</p>

<ol>
<li>
Por cada capa \(q\), con \(1 \leq q \leq k\), sumamos todos los elementos de la matriz de pesos \(\Theta^{q}\), que como sabemos tiene dimensiones \(S_{q} \times S_{q-1}\) 

<li>
Dada la matriz \(\Theta^{(q)}\)

<ol>
<li>
Recorremos cada columna \(i\), con \(1 \leq i \leq S_{q-1}\)

<li>
Recorremos cada elemento \(j\) de la columna \(i\), con \(1 \leq j \leq S_{q}\)

<li>
Sumamos al total cada elemento de la matriz \(\Theta^{(q)}_{ji}\)

</ol>
<li>
Una vez se han sumado todas las matrices de pesos obtenemos un escalar, que multiplicamos por \(\frac{\lambda}{2m}\)

</ol>

<div id="Redes Neuronales-Retropropagación"><h2 id="Retropropagación" class="header"><a href="#Redes Neuronales-Retropropagación">Retropropagación</a></h2></div>

<div id="Redes Neuronales-Retropropagación-Notación"><h4 id="Notación" class="header"><a href="#Redes Neuronales-Retropropagación-Notación">Notación</a></h4></div>

<p>
La salida de cada capa \(q\) es una matriz \(S_q \times m\), donde \(S_q\) denota el número de nodos en la capa \(q\) y \(m\) denota el número de ejemplos. 
</p>

<p>
Como vimos en nuestras figuras, donde se presentaban los cálculos sólo para un ejemplo, en cada capa \(q\) podemos mapear la salida de los \(S_q\) nodos a un vector columna \(S_q \times 1\). 
</p>

<p>
Si generalizamos esto a \(m\) ejemplos tenemos que la salida de cada capa es una matriz \(S_q \times m\). Esto se ilustra en la siguiente imagen:
</p>

<p>
<img src="./assets/nn_multi_ejemplos.svg" alt="Red Neuronal con varios ejemplos" style="width:800px;height:600px" />
</p>

<hr />

<p>
Vamos, ahora a explicar cómo se aplica la retropropagación. Lo primero que debemos tener en cuenta es que este proceso se basa en la misma idea de optimización que la <a href="Regresión Lineal.html">Regresión Lineal</a> y la <a href="Regresión Logística.html">Regresión Logística</a>, es decir, lo que queremos hacer es minimizar el coste, \(J(\Theta)\)
</p>


<p>
Sea \(c\) el número de nodos en la última capa, \(\theta_{it}\) el peso \(t\) del nodo \(i\) de la última capa \(k\), \(a_{ij}^{(k)}\) la salida del nodo \(i\) para el ejemplo \(j\) en la capa \(k\):
</p>
<ol>
<li>
Calculamos el gradiente de la última capa \(k\) como: \(\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \frac{\delta J(\Theta)}{\delta a_{1j}^{(k)}}\frac{\delta a_{1j}^{(k)}}{\delta \theta_{it}^{(k)}}\)

<li>
Calculamos el gradiente en capas intermedias utilizando la regla de la cadena como: \(\frac{\delta J(\Theta)}{\delta \theta_{it}^{(q)}} = \sum_{i=1}^{S_{(q+1)}} \frac{\delta J(\Theta)}{\delta a_{ij}^{(q+1)}}\frac{\delta a_{ij}^{(q+1)}}{\delta a_{ij}^{(q)}}\frac{\delta a_{ij}^{(q)}}{\delta \theta_{it}^{(q)}}\)

</ol>

<p>
Normalmente en las capas intermedias, \(q\), nos referimos al término \(\frac{\delta J(\Theta)}{\delta a_{ij}^{(q+1)}}\) como \(\Delta^{(q+1)}_{ij}\).
</p>

<hr />

<div id="Redes Neuronales-Retropropagación-Derivada de la función de coste"><h3 id="Derivada de la función de coste" class="header"><a href="#Redes Neuronales-Retropropagación-Derivada de la función de coste">Derivada de la función de coste</a></h3></div>

<p>
A continuación explicamos cómo derivar la función de coste (<span id="Redes Neuronales-Retropropagación-Derivada de la función de coste-Paso 1"></span><strong id="Paso 1">Paso 1</strong>).
</p>

<p>
<a href="Derivada de la función de coste.html">Derivada de la función de coste</a>
</p>


<hr />

<div id="Redes Neuronales-Retropropagación-Capas intermedias"><h3 id="Capas intermedias" class="header"><a href="#Redes Neuronales-Retropropagación-Capas intermedias">Capas intermedias</a></h3></div>

<p>
Veamos, ahora, cómo llevar a cabo el <span id="Redes Neuronales-Retropropagación-Capas intermedias-Paso 2"></span><strong id="Paso 2">Paso 2</strong>: ¿cómo calculamos el gradiente (o lo que contribuye el peso \(it\) en el error) para los pesos de las capas intermedias?, es decir, cómo calculamos:
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(q)}} 
\]

<p>
<a href="Derivadas capas intermedias.html">Derivadas capas intermedias</a>
</p>

<hr />

<div id="Redes Neuronales-Retropropagación-Ejemplo de retropropagación"><h3 id="Ejemplo de retropropagación" class="header"><a href="#Redes Neuronales-Retropropagación-Ejemplo de retropropagación">Ejemplo de retropropagación</a></h3></div>

<p>
Por ejemplo, supongamos que tenemos una red con tres capas, entonces \(k=3\), dado un ejemplo \(x_j\). En este caso tenemos que 
</p>

<div id="Redes Neuronales-Retropropagación-Ejemplo de retropropagación-Capa 3"><h4 id="Capa 3" class="header"><a href="#Redes Neuronales-Retropropagación-Ejemplo de retropropagación-Capa 3">Capa 3</a></h4></div>

<p>
La derivada en la última capa, para el único vector de pesos \(\theta^{(3)}_1\) que tiene \(n\) elementos (features o características), es: \(\frac{\delta J(\Theta)}{\delta \theta_{1t}^{(3)}}\), para cada \(t\), \(0 \leq t \leq n\)
</p>

<p>
Como: 
</p>

\begin{align}
J(\Theta) = E^{(3)}(a_1^{(3)}) = E^{(3)}(g(z_1^{(3)})) = E^{(3)}(g(\Theta^{(3)}\cdot a^{(2)}))
\end{align}

<p>
Donde denotamos la función que calcula el error entre lo predicho y la salida real como \(E\), y \(g\) es la función de activación.
</p>

<p>
Entonces, aplicamos la regla de la cadena para cada elemento \(t\) en el vector de pesos:
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{1t}^{(3)}} = \frac{\delta J(\Theta)}{\delta a_1^{(3)}}\frac{\delta a_1^{(3)}}{\delta z_1^{(3)}}\frac{\delta z_1^{(3)}}{\delta \theta_{1t}^{(3)}}
\end{align}

<p>
Si vectorizamos:
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{1}^{(3)}} = \frac{\delta J(\Theta)}{\delta a_1^{(3)}}\frac{\delta a_1^{(3)}}{\delta z_1^{(3)}}\frac{\delta z_1^{(3)}}{\delta \theta_{1}^{(3)}}
\end{align}

<div id="Redes Neuronales-Retropropagación-Ejemplo de retropropagación-Capa 2"><h4 id="Capa 2" class="header"><a href="#Redes Neuronales-Retropropagación-Ejemplo de retropropagación-Capa 2">Capa 2</a></h4></div>

<p>
Si ahora queremos obtener la derivada para uno de los vectores de pesos en la capa \(2\), volvemos a aplicar la regla de la cadena. Tenemos ahora que desestructurar la función de coste todavía más, hasta obtener la expresión que incluye las salidas de la capa \(1\), \(a^{(1)}\).
</p>

\begin{align}
J(\Theta) = E^{(3)}(g(\Theta^{(3)}\cdot a^{(2)})) = E^{(3)}(g(\Theta^{(3)}\cdot g(z^{(2)}))) = E^{(3)}(g(\Theta^{(3)}\cdot g(\Theta^{(2)} \cdot a^{(1)})))
\end{align}

<p>
Sea \(\Delta^{(3)}_{1j}\):
</p>

\begin{align}
\Delta^{(3)}_{1j} = \frac{\delta J(\Theta)}{\delta a_{1j}^{(3)}}\frac{\delta a_{1j}^{(3)}}{\delta z_{1j}^{(3)}}
\end{align}

<p>
Entonces, aplicamos la regla de la cadena para cada nodo \(i\) de la capa \(2\) y para cada elemento \(t\): 
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(2)}} = \sum_{l=1}^{S_{(3)}} \Delta_{lj}^{(3)}\frac{\delta z_{lj}^{(3)}}{\delta a_{lj}^{(2)}}\frac{\delta a_{lj}^{(2)}}{\delta z_{lj}^{(2)}}\frac{\delta z_{lj}^{(2)}}{\delta \theta_{it}^{(2)}} = \Delta_{1j}^{(3)}\frac{\delta z_{1j}^{(3)}}{\delta a_{1j}^{(2)}}\frac{\delta a_{1j}^{(2)}}{\delta z_{1j}^{(2)}}\frac{\delta z_{1j}^{(2)}}{\delta \theta_{it}^{(2)}}
\end{align}

<p>
Si vectorizamos:
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{i}^{(2)}} = \Delta_{j}^{(3)}\frac{\delta z^{(3)}}{\delta a_{j}^{(2)}}\frac{\delta a_{j}^{(2)}}{\delta z_{j}^{(2)}}\frac{\delta z_{j}^{(2)}}{\delta \theta_{i}^{(2)}}
\end{align}

<div id="Redes Neuronales-Retropropagación-Ejemplo de retropropagación-Capa 1"><h4 id="Capa 1" class="header"><a href="#Redes Neuronales-Retropropagación-Ejemplo de retropropagación-Capa 1">Capa 1</a></h4></div>

<p>
Para la capa \(1\), volvemos a expandir la función de coste para ver cómo aplicar la regla de la cadena:
</p>


\begin{align}
J(\Theta) = E^{(3)}(g(\Theta^{(3)}\cdot g(\Theta^{(2)} \cdot a^{(1)}))) = E^{(3)}(g(\Theta^{(3)}\cdot g(\Theta^{(2)} \cdot g(z^{(1)})))) =
\end{align}

\begin{align}
= E^{(3)}(g(\Theta^{(3)}\cdot g(\Theta^{(2)} \cdot g(\Theta^{(1)} x_j))))
\end{align}

<p>
Para simplificar la notación: sea, para cada nodo \(l\) de la capa \(2\)
</p>

\begin{align}
\Delta^{(2)}_{lj} = \Delta_{1j}^{(3)}\frac{\delta z_1^{(3)}}{\delta a_{lj}^{(2)}}\frac{\delta a_{lj}^{(2)}}{\delta z_{lj}^{(2)}}
\end{align}

<hr />

<p>
Aplicamos la regla de la cadena, tal que para cada nodo \(l\) de la capa \(2\):
</p>


\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(1)}} = \sum_{l=1}^{S_{(2)}} \Delta_{lj}^{(2)}\frac{\delta z_{lj}^{(2)}}{\delta a_{lj}^{(1)}}\frac{\delta a_{lj}^{(1)}}{\delta z_{lj}^{(1)}}\frac{\delta z_{lj}^{(1)}}{\delta \theta_{it}^{(1)}}
\end{align}

<p>
Si vectorizamos:
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{i}^{(1)}} = \Delta_{j}^{(2)}\frac{\delta z_{j}^{(2)}}{\delta a_{j}^{(1)}}\frac{\delta a_{j}^{(1)}}{\delta z_{j}^{(1)}}\frac{\delta z_{j}^{(1)}}{\delta \theta_{i}^{(1)}}
\end{align}

<p>
El procedimiento se ilustra en la siguiente figura:
</p>

<p>
<img src="./assets/backprop.svg" alt="Retropropagación" style="width:1000px;height:600px;" />
</p>

<hr />

<div id="Redes Neuronales-Algoritmo"><h2 id="Algoritmo" class="header"><a href="#Redes Neuronales-Algoritmo">Algoritmo</a></h2></div>

<p>
<a href="Partes del algoritmo en python.html">Partes del algoritmo en python</a>
</p>
</div>
  </body>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/highlight.min.js"
  ></script>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/zepto.min.js"
  ></script>
  <script type="text/javascript">
    $("pre").each(function (index, item) {
      $(item).html("<code>" + $(item).html() + "</code>");
    });
    hljs.initHighlightingOnLoad();
  </script>
</html>

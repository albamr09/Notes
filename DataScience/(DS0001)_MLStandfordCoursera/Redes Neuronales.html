<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css" />
    <title>Redes Neuronales</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!-- For LaTeX -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <a
      href="https://albamr09.github.io/"
      style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      "
      >Index</a
    >
    <hr />
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Redes Neuronales"><h1 id="Redes Neuronales" class="header"><a href="#Redes Neuronales">Redes Neuronales</a></h1></div>

<hr />

<div id="Redes Neuronales-Estructura"><h2 id="Estructura" class="header"><a href="#Redes Neuronales-Estructura">Estructura</a></h2></div>

<p>
Dado un ejemplo \(X\), tal que \(X\) es un vector fila \(1 \times 3\):
</p>

\[%align
x = 
\begin{bmatrix}x_1 &amp; x_2 &amp; x_3\end{bmatrix}
\]

<p>
Tenemos que las entradas a la red neuronal con un capa se introducen como sigue:
</p>

<p>
<img src="./assets/nn_ejemplo.svg" alt="Ejemplo Red Neuronal" style="width:350px;height:200px" />
</p>

<p>
De tal manera que obtenemos una salida \(h_\Theta(x)\), esta función se denomina <span id="Redes Neuronales-Estructura-función de activación"></span><strong id="función de activación">función de activación</strong>
</p>

<p>
Supongamos que utilizamos la función sigmoide (problema de clasificación con dos clases):
</p>

\[%align
h_\Theta(X) = \frac{1}{1+e^{-\Theta^TX}}
\]


<div id="Redes Neuronales-Datos de entrada"><h2 id="Datos de entrada" class="header"><a href="#Redes Neuronales-Datos de entrada">Datos de entrada</a></h2></div>

<ul>
<li>
\(X = (x_{ij})\) una matriz \(n \times m\) donde cada \(x_{ij}\) es la característica \(i\) del ejemplo \(j\), tal que 
\begin{align}
X = 
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1m} \\
\cdots &amp; \ddots &amp; \cdots \\
x_{n1} &amp; \cdots &amp; x_{nm} \\
\end{bmatrix} 
\end{align}

</ul>

<p>
<em>Cada columna es un ejemplo</em>
</p>

<p>
<em>En cada fila están los valores de una característica</em>
</p>

<ul>
<li>
\(Y = (y_j)\) es un vector fila \(1\times m\) donde cada \(y_j\) es la salida real para el ejemplo \(j\), tal que:

</ul>

\begin{align}
Y = \begin{bmatrix} y_1 &amp; \cdots &amp; y_m\end{bmatrix} 
\end{align}

<div id="Redes Neuronales-Propagación"><h2 id="Propagación" class="header"><a href="#Redes Neuronales-Propagación">Propagación</a></h2></div>

<div id="Redes Neuronales-Propagación-Notación"><h4 id="Notación" class="header"><a href="#Redes Neuronales-Propagación-Notación">Notación</a></h4></div>

<ul>
<li>
\(k\) es el número de capas de la red

<li>
Para cada capa \(j\), asumimos que la capa tiene \(q\) nodos.

<li>
\(a_i^{(j)}\): Nodo \(i\) en la capa \(j\)

<li>
\(\Theta^{(j)}\): Matriz de pesos de la capa \(j\) a la capa \(j+1\)

<li>
La dimensión de cada \(\Theta^{(j)}\) es \(S_{j+1} \times (S_j + 1)\)

<ul>
<li>
\(S_{j+1}\): número de características/nodos en la capa \(j+1\).

<li>
\((S_j + 1)\): número de características en la capa \(j\) más 1 (término independiente en cada capa)

</ul>
</ul>

<div id="Redes Neuronales-Propagación-Funcionamiento"><h4 id="Funcionamiento" class="header"><a href="#Redes Neuronales-Propagación-Funcionamiento">Funcionamiento</a></h4></div>

<p>
Sean \(k\) el número de capas de la red. Para cada capa \(j\), con \(0 \leq j \leq k\), supongamos que la capa tiene \(q\) nodos. Con \(a^0 = X_l\) (donde \(X_l\) es un ejemplo: un vector columna en \(X\)):
</p>

<p>
Primero se calcula el término intermedio \(z^{(j)}\), que es un vector columna \(q \times 1\):
</p>

\begin{align}
z^{(j)} = \Theta^{(j)} \cdot a^{(j-1)}
\end{align}


<p>
Donde, como ya hemos dicho, \(\Theta\) es la matriz \(q \times p\) de pesos de la capa \(j\), siendo \(p\) el número de nodos en la capa \((j-1)\) más uno (el término independiente).
</p>

<p>
A continuación se aplica sobre \(z^{(j)}\) la función de activación, tal que:
</p>


\begin{align}
a^{(j)} = g(z^{(j)})
\end{align}


<p>
Esto se representa gráficamente en la siguiente imagen:
</p>

<p>
<img src="./assets/nn_ejemplo_1.svg" alt="Ejemplo Red Neuronal" style="width:1050px;height:600px" />
</p>

<p>
Cabe destacar que la función de activación de la última capa suele ser distinta al resto. De todas formas, las funciones de activación no tienen por qué ser iguales, nosotros en estes ejemplos hemos utilizado la función sigmoide, pero depende de la aplicación que se le quiera da a la red neuronal.
</p>

<div id="Redes Neuronales-Propagación-Clasificación múltiple"><h3 id="Clasificación múltiple" class="header"><a href="#Redes Neuronales-Propagación-Clasificación múltiple">Clasificación múltiple</a></h3></div>

<p>
Para crear una red neuronal que permita trabajar con \(c\) clases lo que hacemos es hacer que la red neuronal tenga \(c\) nodos en su capa de salida. Esto se ilustra en la siguiente imagen:
</p>

<p>
<img src="./assets/nn_ejemplo_multi.svg" alt="Ejemplo Red Neuronal Clasificación Múltiple" style="width:950px;height:550px" />
</p>

<p>
De tal manera que ahora, cada salida \(y_j\) será un vector columna \(c\times1\), donde existe un valor por cada categoría, al igual que la hipótesis para el ejemplo \(j\), \(h_\Theta(x_j)\), es un vector columna \(c\times1\).
</p>

<p>
Como podemos ver, los valores de \(y_j\) indican claramente a qué clase pertenece el ejemplo \(j\) (clase 3), mientras que la hipótesis \(h_\Theta(x_j)\) ofrece, para cada clase (columna) la probabilidad de que el ejemplo \(j\) pertenezca a esa clase.
<hr />
</p>

<div id="Redes Neuronales-Función de coste"><h2 id="Función de coste" class="header"><a href="#Redes Neuronales-Función de coste">Función de coste</a></h2></div>

<div id="Redes Neuronales-Función de coste-Notación"><h4 id="Notación" class="header"><a href="#Redes Neuronales-Función de coste-Notación">Notación</a></h4></div>

<p>
Como ya hemos visto en función del número de clases la salida tendrá distinta forma:
</p>

<ul>
<li>
<span id="Redes Neuronales-Función de coste-Notación-Clasificación binaria"></span><strong id="Clasificación binaria">Clasificación binaria</strong>: para cada ejemplo \(j\), \(y_j \in \{0, 1\}\), \(h_\Theta(x_j) \in \mathbb{R}\)

<li>
<span id="Redes Neuronales-Función de coste-Notación-Clasificación múltiple"></span><strong id="Clasificación múltiple">Clasificación múltiple</strong>: para cada ejemplo \(j\), \(y \in \mathbb{R}^c\), \(h_\Theta(x_j) \in \mathbb{R}^c\), donde \(c\) es el número de clases

<li>
Sea \(k\) el número de capas y \(S_i\) el número de nodos en la capa \(i\).

<li>
Sea \(Y=(y_{ij})\) una matriz \(c\times m\), donde \(m\) es el número de ejemplos y cada \(y_{j}\) es el vector columna \(c\times1\) de salida para el ejemplo \(j\).

</ul>

<hr />

<p>
Definimos la función de coste como sigue:
</p>

\begin{align}
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]\right\}
\end{align}

<p>
El primer sumatorio que va de 1 a \(m\) se encarga de calcular el coste para cada ejemplo \(j\). Mientras que el segundo sumatorio, que va de 1 a \(c\), se encarga de calcular el coste para cada nodo de salida.
</p>

<p>
Esta función se aplica sobre los \(k\) nodos en la capa de salida. Tal que, por ejemplo para la figura anterior tenemos que \(c=3\), y la hipótesis tiene los valores:
</p>

\begin{align}
h_\Theta(x_1) = 
\begin{bmatrix}
0.02 \\
0.1 \\
0.88 \\
\end{bmatrix}
\end{align}

<p>
y la salida real para el ejemplo \(x_1\) tiene los valores:
</p>

\begin{align}
y_1 = 
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}
\end{align}

<p>
Entonces la función de coste se calcularía como (observa que esto es sólo para un ejemplo, por lo que obviamos el primer sumatorio):
</p>

\begin{align}
J(\Theta) = - \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]
\end{align}

\begin{align}
J(\Theta) = - \{[(y_{11}\cdot\log(h_\Theta(x_1)_{1})) + (1-y_{11})\cdot\log(1-h_\Theta(x_1)_{1})] +
\end{align}

\begin{align}
+ [(y_{21}\cdot\log(h_\Theta(x_1)_{2})) + (1-y_{21})\cdot\log(1-h_\Theta(x_1)_{2})] +
\end{align}

\begin{align}
+ [(y_{31}\cdot\log(h_\Theta(x_1)_{3})) + (1-y_{31})\cdot\log(1-h_\Theta(x_1)_{3})]\}
\end{align}

<p>
Sustituimos los valores de cada vector:
</p>

\begin{align}
J(\Theta) = - \{ [(0\cdot\log(0.02)) + (1-0)\cdot\log(1-0.02)] +
\end{align}

\begin{align}
+ [(0\cdot\log(0.1)) + (1-0)\cdot\log(1-0.1)] +
\end{align}

\begin{align}
+ [(1\cdot\log(0.88)) + (1-1)\cdot\log(1-0.88)] \} = 
\end{align}

<p>
Calculamos los valores:
</p>

\begin{align}
J(\Theta) = - (\log(0.98) + \log(0.9) + \log(0.88)) 
\end{align}

\begin{align}
J(\Theta) = - (-0.009 - 0.046 -0.056) = - (-0.111) = 0.111
\end{align}

<div id="Redes Neuronales-Función de coste-Regularización"><h4 id="Regularización" class="header"><a href="#Redes Neuronales-Función de coste-Regularización">Regularización</a></h4></div>

<p>
Definimos la función de coste introduciendo regularización como sigue:
</p>

\begin{align}
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]\right\} + \frac{\lambda}{2m} \sum_{q=1}^k \sum_{i=1}^{S_q}\sum_{j=1}^{S_{q+1}} (\theta_{ji}^{(q)})^2
\end{align}


<p>
Antes de nada, recordar que \(S_q\) denota el número de nodos en la capa \(q\). Entonces, el primer término de la función es igual que cuando no se aplicaba regularización. Expliquemos el segundo término. La regularización, en este caso, consiste en sumar todos los pesos de la red neuronal, por lo tanto:
</p>

<ol>
<li>
Por cada capa \(q\), con \(1 \leq q \leq k\), sumamos todos los elementos de la matriz de pesos \(\Theta^{q}\), que como sabemos tiene dimensiones \(S_{q+1} \times S_q\) 

<li>
Dada la matriz \(\Theta^{(q)}\)

<ol>
<li>
Recorremos cada columna \(i\), con \(1 \leq i \leq S_q\)

<li>
Recorremos cada elemento \(j\) de la columna \(i\), con \(1 \leq j \leq S_{q+1}\)

<li>
Sumamos al total cada elemento de la matriz \(\Theta^{(j)}_{ji}\)

</ol>
<li>
Una vez se han sumado todas las matrices de pesos obtenemos un escalar, que multiplicamos por \(\frac{\lambda}{2m}\)

</ol>

<div id="Redes Neuronales-Retropropagación"><h2 id="Retropropagación" class="header"><a href="#Redes Neuronales-Retropropagación">Retropropagación</a></h2></div>

<div id="Redes Neuronales-Retropropagación-Notación"><h4 id="Notación" class="header"><a href="#Redes Neuronales-Retropropagación-Notación">Notación</a></h4></div>

<ul>
<li>
\(\delta^{(q)}_i\) denota el error del nodo \(i\) en la capa \(q\)

<li>
\(a^{(q)}_i\) denota la salida del nodo \(i\) en la capa \(q\)

</ul>

<p>
La salida de cada capa \(q\) es una matriz \(S_q \times m\), donde \(S_q\) denota el número de nodos en la capa \(q\) y \(m\) denota el número de ejemplos. 
</p>

<p>
Como vimos en nuestras figuras, donde se presentaban los cálculos sólo para un ejemplo, en cada capa \(q\) podemos mapear la salida de los \(S_q\) nodos a un vector columna \(S_q \times 1\). 
</p>

<p>
Si generalizamos esto a \(m\) ejemplos tenemos que la salida de cada capa es una matriz \(S_q \times m\). Esto se ilustra en la siguiente imagen:
</p>

<p>
<img src="./assets/nn_multi_ejemplos.svg" alt="Red Neuronal con varios ejemplos" style="width:800px;height:600px" />
</p>

<hr />

<p>
Vamos, ahora a explicar cómo se aplica la retropropagación. Lo primero que debemos tener en cuenta es que este proceso se basa en la misma idea de optimización que la <a href="Regresión Lineal.html">Regresión Lineal</a> y la <a href="Regresión Logística.html">Regresión Logística</a>, es decir, lo que queremos hacer es minimizar el coste, \(J(\Theta)\)
</p>

<p>
El problema es que debemos derivar \(\frac{\delta J(\Theta)}{\delta \Theta}\), donde \(\Theta\) ya no es una matriz, si no que es un tensor, es decir tenemos que calcular \(\frac{\delta J(\Theta)}{\delta \theta^{(k)}_{it}}\). Entonces, sabemos que la función de coste:
</p>

\[%align
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m\sum_{i=1}^c (y_{ij}\cdot \log(h_\Theta(x_j)_i)) + [(1-y_{ij})\log(1-h_\Theta(x_j)_i)]\right\}
\]

<p>
Donde \(\theta_{it}^{(k)}\) es el peso que conecta el nodo \(i\) de la capa \(k\) con el nodo \(t\) de la capa \((k-1)\), es decir, es el elemento en la fila \(i\) columna \(t\) de la matriz de pesos de la capa \(k\), \(\Theta^{(k)}\).
</p>


<div id="Redes Neuronales-Retropropagación-Derivada de la función de coste"><h3 id="Derivada de la función de coste" class="header"><a href="#Redes Neuronales-Retropropagación-Derivada de la función de coste">Derivada de la función de coste</a></h3></div>

<p>
Por la regla de la cadena, separamos la derivada de la función del coste en función de los pesos en dos términos:
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \sum_{j=1}^m \sum_{i=1}^c \frac{\delta E^{(k)}}{\delta a_{ij}^{(k)}} \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}
\end{align}


<div id="Redes Neuronales-Retropropagación-Capa de salida"><h3 id="Capa de salida" class="header"><a href="#Redes Neuronales-Retropropagación-Capa de salida">Capa de salida</a></h3></div>

<p>
Procedemos a calcular la derivada:
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \frac{\delta}{\delta \theta_{it}^{(k)}} \left(- \frac{1}{m}\right) \left\{ \sum_{j=1}^m\sum_{i=1}^c (y_{ij}\cdot \log(h_\Theta(x_j)_i)) + [(1-y_{ij})\log(1-h_\Theta(x_j)_i)]\right\}
\]

<p>
Sacamos el término constante de la derivada y aplicamos la propiedad: "La derivada de una suma equivale a la suma de las derivadas"
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c \frac{\delta}{\delta \theta_{it}^{(k)}} \left\{(y_{ij}\cdot \log(h_\Theta(x_j)_i)) + [(1-y_{ij})\log(1-h_\Theta(x_j)_i)]\right\}
\]

<p>
Sea \(h_\Theta(x_j) = a^{(k)}_j\), es decir la salida de la última capa para el ejemplo \(j\).
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c \frac{\delta}{\delta \theta_{it}^{(k)}} \left\{(y_{ij}\cdot \log(a^{(k)}_{ij})) + [(1-y_{ij})\log(1-a^{(k)}_{ij})]\right\}
\]


<p>
Sacaremos el término \(y_{ij}\) de la derivada y juntemos todas las expresiones:
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c  \left\{y_{ij} \left(\frac{\delta}{\delta \theta_{it}^{(k)}} \log(a^{(k)}_{ij}) \right) + (1-y_{ij}) \left(\frac{\delta}{\delta \theta_{it}^{(k)}} \log(1-a^{(k)}_{ij})\right)\right\}
\]

<p>
Aplicamos la regla de la cadena sobre el logaritmo:
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c  \left\{y_{ij} \left(\frac{\delta \log(a_{ij}^{(k)})}{\delta a_{ij}^{(k)}}  \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} \right) + (1-y_{ij}) \left(\frac{\delta \log(1-a^{(k)}_{ij})}{\delta (1-a^{(k)}_{ij})} \frac{\delta (1-a^{(k)}_{ij})}{\delta \theta_{it}^{(k)}} \right)\right\}
\]

<p>
Como sabemos:
</p>

<ol>
<li>
\(\frac{\delta (1)}{\delta \theta_{it}^{(k)}} = 0\), entonces

<li>
\(\frac{\delta(1-a_{ij}^{(k)})}{\delta \theta_{it}^{(k)}} = \frac{\delta (1)}{\delta \theta_{it}^{(k)}} - \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} = 0 + (-1) \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}\)

</ol>

<p>
Entonces
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c  \left\{y_{ij} \left(\frac{\delta \log(a_{ij}^{(k)})}{\delta a_{ij}^{(k)}}  \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} \right) + (1-y_{ij}) \left((-1)\frac{\delta \log(1-a^{(k)}_{ij})}{\delta (1-a^{(k)}_{ij})} \frac{\delta a^{(k)}_{ij}}{\delta \theta_{it}^{(k)}} \right)\right\}
\]

<p>
Sacamos \(\frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}\) como factor común y aplicamos el \((-1)\):
</p>


\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c  \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}\left\{y_{ij} \frac{\delta \log(a_{ij}^{(k)})}{\delta a_{ij}^{(k)}} -  \left((1-y_{ij})\frac{\delta \log(1-a^{(k)}_{ij})}{\delta (1-a^{(k)}_{ij})} \right)\right\}
\]

<p>
Sustituimos \(\frac{\delta E^{(k)}}{\delta a_{ij}^{(k)}} = \left\{y_{ij} \frac{\delta \log(a_{ij}^{(k)})}{\delta a_{ij}^{(k)}} -  \left((1-y_{ij})\frac{\delta \log(1-a^{(k)}_{ij})}{\delta (1-a^{(k)}_{ij})} \right)\right\}\)
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c \frac{\delta E^{(k)}}{\delta a_{ij}^{(k)}} \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} 
\]

<p>
Si resolvemos las derivadas de los logaritmos obtenemos:
</p>

\[%align
\frac{\delta E^{(k)}}{\delta a_{ij}^{(k)}} = y_{ij} \frac{\delta \log(a_{ij}^{(k)})}{\delta a_{ij}^{(k)}} - (1-y_{ij})\frac{\delta \log(1-a_{ij}^{(k)})}{\delta (1-a_{ij}^{(k)})}
\]

<hr />

<p>
Nos centraremos ahora en la derivada que nos falta \(\frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}\):
</p>

<p>
Sabemos que, vectorizando la operación, \(a^{(k)}_j = g(z^{(k)}_j)\), donde \(g\) es la función de activación (en este caso sigmoide).
</p>

<p>
Además:
</p>

\[%align
z^{(k)}_j = \Theta^{k} \cdot a^{(k-1)}_j
\]

<p>
Por lo tanto, para cada nodo \(i\) en la última capa \(k\):
</p>

\[%align
z^{(k)}_{ij} = \sum_{l=1}^{S_{(k-1)}} \theta^{(k)}_{il} \cdot a^{(k-1)}_{lj}
\]

<p>
Donde \(S_{(k-1)}\) es el número de nodos en la capa \(k-1\). Entonces, aplicamos de nuevo la regla de la cadena:
</p>

\[%align
\frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} =  \frac{\delta g(z_{ij}^{(k)})}{\delta z_{ij}^{(k)}} \frac{\delta z_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}
\]

<p>
Resolvemos la derivada para el segundo término:
</p>

\[%align
\frac{\delta z_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} = \sum_{l=1}^{S_{(k-1)}} \frac{\delta}{\delta \theta_{it}^{(k)}} \theta^{(k)}_{il} \cdot a^{(k-1)}_{lj}
\]

<p>
Tal que:
</p>

\[%align
\frac{\delta}{\delta \theta_{it}^{(k)}} \theta^{(k)}_{il} \cdot a^{(k-1)}_{lj} =
\begin{cases}
a_{lj}^{(k-1)}, &amp; t = l \\
0, &amp; t \neq l \\
\end{cases}
\]

<p>
Por lo tanto, como sólo hay un \(l\) con \(l = t\) donde \(1 \leq l \leq S_{(k-1)}\), entonces:
</p>

\[%align
\frac{\delta z_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} = a_{lj}^{(k-1)} = a_{tj}^{(k-1)}
\]

<p>
Juntamos ambos términos de la derivada inicial, con \(\frac{\delta g(z_{ij}^{(k)})}{\delta z_{ij}^{k}} = \sigma'(z_{ij}^{(k)})\)
</p>

\[%align
\frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} =  \frac{\delta g(z_{ij}^{(k)})}{\delta z_{ij}^{(k)}} \frac{\delta z_{ij}^{(k)}}{\delta \theta_{it}^{(k)}} = \sigma'(z_{ij}^{(k)}) a_{tj}^{(k-1)}
\]

<hr />

<p>
Vamos a resumir lo que tenemos hasta ahora. Por la regla de la cadena, separamos la derivada de la función del coste en función de los pesos en dos términos:
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \sum_{j=1}^m \sum_{i=1}^c \frac{\delta E^{(k)}}{\delta a_{ij}^{(k)}} \frac{\delta a_{ij}^{(k)}}{\delta \theta_{it}^{(k)}}
\end{align}

<p>
Si sustituimos ambos términos, para la capa de salida \(k\):
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \left(- \frac{1}{m}\right)  \sum_{j=1}^m\sum_{i=1}^c  \sigma'(z_{ij}^{(k)}) a_{tj}^{(k-1)}\left\{ \frac{y_{ij}}{a_{ij}^{(k)}} -  \left(\frac{(1-y_{ij})}{(1-a^{(k)}_{ij})} \right)\right\}
\]

<div id="Redes Neuronales-Retropropagación-Capas intermedias"><h3 id="Capas intermedias" class="header"><a href="#Redes Neuronales-Retropropagación-Capas intermedias">Capas intermedias</a></h3></div>

<p>
Pero, ¿cómo calculamos el gradiente para los pesos de las capas intermedias?, es decir, cómo calculamos:
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(q)}} 
\]

<p>
Donde \(q\) denota la capa, con \(1 \leq q \leq (k-1)\). Bien, haciendo referencia a la sección de notación, tenemos que \(\delta_i^{(q)}\) es el error del nodo \(i\) en la capa \(q\). Sea \(q = k - 1\), tenemos que:
</p>


<div id="Redes Neuronales-Algoritmo"><h2 id="Algoritmo" class="header"><a href="#Redes Neuronales-Algoritmo">Algoritmo</a></h2></div>
</div>
  </body>
</html>

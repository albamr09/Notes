<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css" />
    <title>Neural Networks</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!-- For LaTeX -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <!-- Code Highlight -->
    <link
      rel="Stylesheet"
      type="text/css"
      href="https://albamr09.github.io/atom-one-light.min.css"
    />
  </head>
  <body>
    <a
      href="https://albamr09.github.io/"
      style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      "
      >Index</a
    >
    <hr />
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Neural Networks"><h1 id="Neural Networks" class="header"><a href="#Neural Networks">Neural Networks</a></h1></div>

<hr />

<div id="Neural Networks-Architecture"><h2 id="Architecture" class="header"><a href="#Neural Networks-Architecture">Architecture</a></h2></div>

<ul>
<li>
<span id="Neural Networks-Architecture-Input"></span><strong id="Input">Input</strong>: Given any input \(X\) the first thing we do is flatten it. For example if \(X\) is a rgb image of \(64 \times 64\), then \(X \in \mathbb{R}^{64 \times 64 \times 3}\) (for each of the \(64 \times 64\) pixels we have three color channels: red, green, blue), is flattened into a vector in \(\mathbb{R}^{(64*64*3) \times 1}\)

<li>
<span id="Neural Networks-Architecture-Neuron"></span><strong id="Neuron">Neuron</strong>: is an operation that has two parts:

<ul>
<li>
Linear part: we denote the linear part like \(z^{[i]}\), where \(i\) is the current layer.

</ul>
</ul>
  
<p>
  <img src="./assets/neuron.svg" alt="Linear Part Neuron Example" style="transform: translate(22vw, 0)" />
</p>
<ul>
<li>
Activation part

</ul>
  
<p>
  <img src="./assets/neuron_activation.svg" alt="Activation Part Neuron Example" style="transform: translate(22vw, 0)" />
</p>
<ul>
<li>
<span id="Neural Networks-Architecture-Layer"></span><strong id="Layer">Layer</strong>: a layer is a compound of neurons that are not connected with each other.

</ul>
  
<p>
  <img src="./assets/layer.svg" alt="Layer Example" style="transform: translate(22vw, 0)" />
</p>

<div id="Neural Networks-Architecture-Algorithm"><h3 id="Algorithm" class="header"><a href="#Neural Networks-Architecture-Algorithm">Algorithm</a></h3></div>

<p>
The principal steps of the algorithm are:
</p>

<ol>
<li>
Initialize the weights \(w\) and biases \(b\) randomly

<li>
Find the optimal \(w, b\)

<li>
Use the optimized \(w, b\) to predict the output by using the formula \(\hat{y} = \sigma(wx +b)\)

</ol>

<div id="Neural Networks-Architecture-Output Layer"><h3 id="Output Layer" class="header"><a href="#Neural Networks-Architecture-Output Layer">Output Layer</a></h3></div>

<div id="Neural Networks-Architecture-Output Layer-Sigmoid"><h4 id="Sigmoid" class="header"><a href="#Neural Networks-Architecture-Output Layer-Sigmoid">Sigmoid</a></h4></div>

<p>
The output layer will be different depending on the problem we are tackling. For example if we want to discriminate between 3 classes then the output layer could be as follows:
</p>

<p>
<img src="./assets/nn_multiclass.svg" alt="NN with Multiclass" style="transform: translate(7vw, 0)" />
</p>

<p>
So now the output is a vector \(\hat{y} \in \mathbb{R}^{c \times 1}\) where \(c\) is the number of classes.
</p>

<div id="Neural Networks-Architecture-Output Layer-Softmax"><h4 id="Softmax" class="header"><a href="#Neural Networks-Architecture-Output Layer-Softmax">Softmax</a></h4></div>

<p>
The previous classifier allows for outputting multiples classes in the result, that is we can obtain a predicted output of the form \(\hat{y} = \begin{bmatrix} 1 \\1 \\ 0 \end{bmatrix}\). What if we want to add a constraint such that only one class can be predicted. Then we use the softmax function as the activation function on the output layer:
</p>

<p>
<img src="./assets/nn_multiclass_softmax.svg" alt="NN with Multiclass using Softmax" style="transform: translate(7vw, 0)" />
</p>

<p>
Thus, instead of a probability for each class what we obtain is a probability distribution for all the classes.
</p>

<div id="Neural Networks-Forward Propagation"><h2 id="Forward Propagation" class="header"><a href="#Neural Networks-Forward Propagation">Forward Propagation</a></h2></div>

<div id="Neural Networks-Optimization Problem"><h2 id="Optimization Problem" class="header"><a href="#Neural Networks-Optimization Problem">Optimization Problem</a></h2></div>
</div>
  </body>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/highlight.min.js"
  ></script>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/zepto.min.js"
  ></script>
  <script type="text/javascript">
    $("pre").each(function (index, item) {
      $(item).html("<code>" + $(item).html() + "</code>");
    });
    hljs.initHighlightingOnLoad();
  </script>
</html>

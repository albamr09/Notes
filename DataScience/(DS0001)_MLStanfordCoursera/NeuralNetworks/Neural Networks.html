<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css" />
    <title>Neural Networks</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!-- For LaTeX -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <!-- Code Highlight -->
    <link
      rel="Stylesheet"
      type="text/css"
      href="https://albamr09.github.io/atom-one-light.min.css"
    />
  </head>
  <body>
    <a
      href="https://albamr09.github.io/"
      style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      "
      >Index</a
    >
    <hr />
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Neural Networks"><h1 id="Neural Networks" class="header"><a href="#Neural Networks">Neural Networks</a></h1></div>

<hr />

<div id="Neural Networks-Architecture"><h2 id="Architecture" class="header"><a href="#Neural Networks-Architecture">Architecture</a></h2></div>

<ul>
<li>
<span id="Neural Networks-Architecture-Input"></span><strong id="Input">Input</strong>: Given any input \(X\) the first thing we do is flatten it. For example if \(X\) is a rgb image of \(64 \times 64\), then \(X \in \mathbb{R}^{64 \times 64 \times 3}\) (for each of the \(64 \times 64\) pixels we have three color channels: red, green, blue), is flattened into a vector in \(\mathbb{R}^{(64*64*3) \times 1}\)

<li>
<span id="Neural Networks-Architecture-Neuron"></span><strong id="Neuron">Neuron</strong>: is an operation that has two parts:

<ul>
<li>
Linear part: we denote the linear part like \(z^{[i]}\), where \(i\) is the current layer.

</ul>
</ul>
  
<p>
  <img src="./assets/neuron.svg" alt="Linear Part Neuron Example" style="transform: translate(22vw, 0)" />
</p>
<ul>
<li>
Activation part

</ul>
  
<p>
  <img src="./assets/neuron_activation.svg" alt="Activation Part Neuron Example" style="transform: translate(22vw, 0)" />
</p>
<ul>
<li>
<span id="Neural Networks-Architecture-Layer"></span><strong id="Layer">Layer</strong>: a layer is a compound of neurons that are not connected with each other.

</ul>
  
<p>
  <img src="./assets/layer.svg" alt="Layer Example" style="transform: translate(22vw, 0)" />
</p>

<div id="Neural Networks-Architecture-Algorithm"><h3 id="Algorithm" class="header"><a href="#Neural Networks-Architecture-Algorithm">Algorithm</a></h3></div>

<p>
The principal steps of the algorithm are:
</p>

<ol>
<li>
Initialize the weights \(w\) and biases \(b\) randomly

<li>
Find the optimal \(w, b\)

<li>
Use the optimized \(w, b\) to predict the output by using the formula \(\hat{y} = \sigma(wx +b)\)

</ol>

<div id="Neural Networks-Architecture-Output Layer"><h3 id="Output Layer" class="header"><a href="#Neural Networks-Architecture-Output Layer">Output Layer</a></h3></div>

<div id="Neural Networks-Architecture-Output Layer-Sigmoid"><h4 id="Sigmoid" class="header"><a href="#Neural Networks-Architecture-Output Layer-Sigmoid">Sigmoid</a></h4></div>

<p>
The output layer will be different depending on the problem we are tackling. For example if we want to discriminate between 3 classes then the output layer could be as follows:
</p>

<p>
<img src="./assets/nn_multiclass.svg" alt="NN with Multiclass" style="transform: translate(7vw, 0)" />
</p>

<p>
So now the output is a vector \(\hat{y} \in \mathbb{R}^{c \times 1}\) where \(c\) is the number of classes.
</p>

<div id="Neural Networks-Architecture-Output Layer-Softmax"><h4 id="Softmax" class="header"><a href="#Neural Networks-Architecture-Output Layer-Softmax">Softmax</a></h4></div>

<p>
The previous classifier allows for outputting multiples classes in the result, that is we can obtain a predicted output of the form \(\hat{y} = \begin{bmatrix} 1 \\1 \\ 0 \end{bmatrix}\). What if we want to add a constraint such that only one class can be predicted. Then we use the softmax function as the activation function on the output layer:
</p>

<p>
<img src="./assets/nn_multiclass_softmax.svg" alt="NN with Multiclass using Softmax" style="transform: translate(7vw, 0)" />
</p>

<p>
Thus, instead of a probability for each class what we obtain is a probability distribution for all the classes.
</p>

<div id="Neural Networks-Architecture-Output Layer-ReLU"><h4 id="ReLU" class="header"><a href="#Neural Networks-Architecture-Output Layer-ReLU">ReLU</a></h4></div>

<p>
On linear regression we do not want the activation function to be linear, because then the whole network becomes a very large linear regression. Instead we use as an activation function the ReLU function (Rectified Linear Unit), whose output is zero if the input value is negative and linear otherwise.
</p>

<p>
<img src="./assets/relu.svg" alt="ReLu Function" style="transform: translate(14vw, 0)" />
</p>

<div id="Neural Networks-Architecture-Output Layer-Loss Function"><h4 id="Loss Function" class="header"><a href="#Neural Networks-Architecture-Output Layer-Loss Function">Loss Function</a></h4></div>

<p>
The loss function when using the sigmoid function on the output layer is as follows:
</p>

\begin{align}
\mathcal{L} = - \frac{1}{Q} \sum_{k=1}^Q [y^{(k)} \log(\hat{y}^{(k)}) + (1- y^{(k)})\log(1-\hat{y}^{(k)})]
\end{align}

<p>
Where \(\hat{y}^{(k)}\) are the predicted values and \(Q\) is the total number of neurons on the output layer.
</p>

<hr />

<p>
However, if we use the softmax function as the activation function on the last layer we have to use a different derivative because this function does depend on the outputs of the other neurons. Thus, we make use of a function called cross entropy loss:
</p>

\begin{align}
\mathcal{L}_{CE} = - \sum_{k=1}^Q y^{(k)} \log(\hat{y}^{(k)})
\end{align}

<hr />

<p>
For linear regression we use as the loss function the L1-Norm or the L2-Norm. The latter is defined as follows:
</p>

\begin{align}
\mathcal{L} = || \hat{y} - y ||_2^2
\end{align}

<div id="Neural Networks-Forward Propagation"><h2 id="Forward Propagation" class="header"><a href="#Neural Networks-Forward Propagation">Forward Propagation</a></h2></div>

<p>
The forward propagation equations are the following:
</p>

\begin{align}
z^{[i]} = w^{[i]} a^{[i-1]} + b^{[i]} \tag{1}
\end{align}

<p>
Where \(i\) is the layer with \(i \geq 1\), and the first layer equals the input matrix, that is \(a^{[0]} = X\). By applying the activation function over \((1)\):
</p>

\begin{align}
a^{[i]} = g(z^{[i]})
\end{align}

<p>
Where \(g\) is the activation function (e.g \(\sigma(z^{[i]})\)).
</p>

<p>
Now, what are the shapes of these matrices?
</p>

<ul>
<li>
\(z^{[i]} \in \mathbb{R}^{S_i \times m}\)

<li>
\(a^{[i]} \in \mathbb{R}^{S_i \times m}\)

</ul>

<p>
Where \(S_i\) is the number of neurons on the ith layer and \(m\) is the number of examples. Note that the shape of the final layer changes depending on the task. So if \(K\) is the number of layers:
</p>

<ul>
<li>
In linear regression: \(\hat{y} = a^{[K]} \in \mathbb{R}^{1 \times m}\)

<li>
In multi-class classification: \(\hat{y} = a^{[K]} \in \mathbb{R}^{c \times m}\), where \(c\) is the number of classes.

</ul>

<p>
Also the shape of the weights are \(w[i] \in \mathbb{R}^{S_i \times S_{i-1}}\), that is, this matrix is compatible with the outputs of the previous layer (\(a^{[i-1]} \in \mathbb{R}^{S_{i-1} \times m}\)) and the linear part of the next layer (\(z^{[i]} \in \mathbb{R}^{S^i \times m}\)).
</p>

<p>
However, the bias are \(b^{[i]} \in \mathbb{R}^{S^i \times 1}\), therefore we cannot perform an element wise summation because the shape of \((w^{[i]} a^{[i-1]}) \in \mathbb{R}^{S_i \times m}\) and \(b^{[i]}\) are not compatible. To avoid this problem we apply a technique called broadcasting to \(b\), such that we replicate \(b^{[i]}\) \(m\) times:
</p>

\begin{align}
\hat{b}^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
b^{[i]} &amp; b^{[i]} &amp; \cdots &amp; b^{[i]} \\
| &amp; | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

<hr />

<p>
To sum up, the shapes of the data and the parameters on each layer \(i\) are:
</p>

<div id="Neural Networks-Forward Propagation-Parameters"><h4 id="Parameters" class="header"><a href="#Neural Networks-Forward Propagation-Parameters">Parameters</a></h4></div>

\begin{align}
\hat{b}^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
b^{[i]} &amp; b^{[i]} &amp; \cdots &amp; b^{[i]} \\
| &amp; | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

\begin{align}
w^{[i]} = \begin{bmatrix}
| &amp; | &amp; \cdots &amp; | \\
w^{[i](1)} &amp; w^{[i](2)} &amp; \cdots &amp; w^{[i](S_{i-1})} \\
| &amp; | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times S_{i-1}}
\end{align}

<div id="Neural Networks-Forward Propagation-Outputs"><h4 id="Outputs" class="header"><a href="#Neural Networks-Forward Propagation-Outputs">Outputs</a></h4></div>

<p>
Note that for each example \(j\) on layer \(i\) \(z^{[i](j)} = (w^{[i]} a^{[i-1](j)} + \hat{b}^{[i]})\), then:
</p>

\begin{align}
z^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
z^{[i](1)} &amp; z^{[i](2)} &amp; \cdots &amp; z^{[i](m)} \\
| &amp;  | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

\begin{align}
a^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
g(z^{[i](1)}) &amp; g(z^{[i](2)}) &amp; \cdots &amp; g(z^{[i](m)}) \\
| &amp;  | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

<div id="Neural Networks-Forward Propagation-Graphical Representation"><h4 id="Graphical Representation" class="header"><a href="#Neural Networks-Forward Propagation-Graphical Representation">Graphical Representation</a></h4></div>

<p>
Now we present a small example of how forward propagation works on neural networks:
</p>

<p>
<img src="./assets/nn_forward_propagation.svg" alt="Forward Propagation Neural Network" style="width: 1000px" />
</p>

<div id="Neural Networks-Optimization Problem"><h2 id="Optimization Problem" class="header"><a href="#Neural Networks-Optimization Problem">Optimization Problem</a></h2></div>

<div id="Neural Networks-Optimization Problem-Loss Function"><h3 id="Loss Function" class="header"><a href="#Neural Networks-Optimization Problem-Loss Function">Loss Function</a></h3></div>

<div id="Neural Networks-Optimization Problem-Optimization"><h3 id="Optimization" class="header"><a href="#Neural Networks-Optimization Problem-Optimization">Optimization</a></h3></div>
</div>
  </body>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/highlight.min.js"
  ></script>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/zepto.min.js"
  ></script>
  <script type="text/javascript">
    $("pre").each(function (index, item) {
      $(item).html("<code>" + $(item).html() + "</code>");
    });
    hljs.initHighlightingOnLoad();
  </script>
</html>

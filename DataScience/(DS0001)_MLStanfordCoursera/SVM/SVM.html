<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css" />
    <title>SVM</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!-- For LaTeX -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <!-- Code Highlight -->
    <link
      rel="Stylesheet"
      type="text/css"
      href="https://albamr09.github.io/atom-one-light.min.css"
    />
  </head>
  <body>
    <a
      href="https://albamr09.github.io/"
      style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      "
      >Index</a
    >
    <hr />
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="SVM"><h1 id="SVM" class="header"><a href="#SVM">SVM</a></h1></div>

<hr />

<ol>
<li>
<a href="SVM.html#SVM-Notation">Notation</a>

<li>
<a href="SVM.html#SVM-Functional Margin">Functional Margin</a>

<li>
<a href="SVM.html#SVM-Geometric Margin">Geometric Margin</a>

<li>
<a href="SVM.html#SVM-Relationship between Functional Margin and Geometric Margin">Functional and Geometric Margin</a>

<li>
<a href="SVM.html#SVM-Optimal Margin Classifier">Optimal Margin Classifier</a>

<li>
<a href="SVM.html#SVM-SVM">SVM</a>

<ol>
<li>
<a href="SVM.html#SVM-SVM-Kernels">Kernels</a>

</ol>
</ol>

<hr />

<p>
The <span id="SVM-Support Vector Machine"></span><strong id="Support Vector Machine">Support Vector Machine</strong> allows you to find potential non-linear decision boundaries:
</p>

<p>
<img src="./assets/non_linear_boundary_SVM.png" alt="Non-Linear Boundary with SVM" style="transform: translate(28vw, 0%);" />
</p>

<p>
<span id="SVM-SVM"></span><strong id="SVM">SVM</strong> provides an algorithm that:
</p>

<ul>
<li>
Maps a vector of features to a vector of features of a much higher dimension (manually picking the new features is difficult, that is why we automate it with these types of algorithms)
\begin{align}
\begin{bmatrix}
x_1 \\
x_2 \\
\end{bmatrix} \rightarrow 
\begin{bmatrix}
x_1 \\
x_2 \\
x_1^2 \\
x_2^2 \\
x_1\cdot x_2 \\
\vdots
\end{bmatrix}
\end{align}

</ul>

<ul>
<li>
Applies a linear classifier over the high dimensional features (<em>Note</em>: if you apply logistic regression to high dimensional vectors then it can learn non-linear decision boundaries)

</ul>

<div id="SVM-Notation"><h2 id="Notation" class="header"><a href="#SVM-Notation">Notation</a></h2></div>

<ul>
<li>
Labels: \(y^{(i)} \in \{-1, +1\}\)

<li>
Now the hypothesis outputs a \(1\) or a \(-1\), which means:

</ul>

\begin{align}
g(z) = 
\begin{cases}
1, &amp; \text{ if } z \geq 0 \\
0, &amp; \text{ otherwise } \\
\end{cases}
\end{align}

<p>
That is, now instead of a smooth transition of probabilities from zero to one, we have a hard transition between \(1\) and \(-1\).
</p>

<ul>
<li>
Weights: now the weights \(\Theta \in \mathbb{R}^{(n+1)}\), where \(\theta_0 = 1\) are divided into: \(w \in \mathbb{R}^{(n)}\) and \(b \in \mathbb{R}\). Thus we drop the convention of assigning \(x_0 = 1\).

<li>
Also now the hypothesis function is defined as: \(h_{w,b}(x) = g(w^Tx + b) = g((\sum_{i=1}^n w_i x) + b)\)

</ul>

<div id="SVM-Functional Margin"><h2 id="Functional Margin" class="header"><a href="#SVM-Functional Margin">Functional Margin</a></h2></div>

<p>
<a href="Functional Margin.html">Functional Margin</a>
</p>


<div id="SVM-Geometric Margin"><h2 id="Geometric Margin" class="header"><a href="#SVM-Geometric Margin">Geometric Margin</a></h2></div>

<p>
<a href="Geometric Margin.html">Geometric Margin</a>
</p>


<div id="SVM-Relationship between Functional Margin and Geometric Margin"><h2 id="Relationship between Functional Margin and Geometric Margin" class="header"><a href="#SVM-Relationship between Functional Margin and Geometric Margin">Relationship between Functional Margin and Geometric Margin</a></h2></div>

<p>
As you may have picked up we can stablish an equality between both margins:
</p>

\begin{align}
\gamma^{(i)} = \frac{\hat{\gamma}^{(i)}}{||w||}
\end{align}

<div id="SVM-Optimal Margin Classifier"><h2 id="Optimal Margin Classifier" class="header"><a href="#SVM-Optimal Margin Classifier">Optimal Margin Classifier</a></h2></div>

<p>
<a href="Optimal Margin Classifier.html">Optimal Margin Classifier</a>
</p>

<div id="SVM-SVM"><h2 id="SVM" class="header"><a href="#SVM-SVM">SVM</a></h2></div>

<div id="SVM-SVM-Kernels"><h3 id="Kernels" class="header"><a href="#SVM-SVM-Kernels">Kernels</a></h3></div>

<p>
To apply kernels first we will lay out the kernel trick:
</p>

<ul>
<li>
Write the algorithm in terms of the inner products of the training examples \(\langle x^{(i)}, x^{(j)} \rangle=(\langle x, z \rangle)\) 

<li>
Let there be a mapping \(x \rightarrow \phi(x)\), where \(\phi(x)\) is a high dimensional feature vector.

<li>
Find a way to compute \(K(x, z) = \phi(x)^T\phi(z)\), even if \(x, z\) are very high dimensional features vectors (which would be very computationally expensive). Where \(K(x, z)\) is denoted as the kernel function

<li>
Replace \(\langle x, z \rangle\) with \(K(x, z)\)

</ul>

<div id="SVM-SVM-Kernels-Kernel Examples"><h4 id="Kernel Examples" class="header"><a href="#SVM-SVM-Kernels-Kernel Examples">Kernel Examples</a></h4></div>

<ul>
<li>
Given \(x, z \in \mathbb{R}^n\), where:

</ul>

\begin{align}
x = \begin{bmatrix}
x_1 \\
\vdots \\ 
x_n \\
\end{bmatrix}
\end{align}

<p>
We define the mapping \(\phi(x) \in \mathbb{R}^{n^2}\) as follows:
</p>

\begin{align}
\phi(x) = \begin{bmatrix}
x_ix_i \\
\end{bmatrix}
\end{align}

<p>
\(\forall i, j\) with \(1 \leq i,j \leq n\)
</p>

<p>
So we have 
</p>

\begin{align}
K(x, z) = \phi(x)^T \phi(z) = \sum_{i=1}^{n^2} \phi(x)_i \phi(z)_i = \sum_{i=1}^n \sum_{j=1}^n (x_ix_j) (z_iz_j)
\end{align}

<p>
Which would take \(O(n^2)\) time to compute. But, observe that:
</p>

\begin{align}
(x^Tz)^2 = (x^Tz)^T(x^Tz) = \sum_{i=1}^n\sum_{j=1}^n (x_iz_i)(x_jz_j) = \sum_{i=1}^n\sum_{j=1}^n (x_ix_j)(z_iz_j)
\end{align}

<p>
whick takes \(O(n)\) time to compute.
</p>

<p>
So we conclude that the kernel can be defined as \(K(x, z) = (x^Tz)^n\)
</p>

<hr />

<ul>
<li>
Given \(x, z \in \mathbb{R}^n\)

<ul>
<li>
\(K(x, z) = (x^Tz + c)^2\)

<li>
Where the mapping function \(\phi\) is defined as: given

</ul>
</ul>
  
\begin{align}
x = \begin{bmatrix}
x_1 \\
x_2 \\
\end{bmatrix}
\end{align}

<p>
Then:
</p>

\begin{align}
\phi(x) = \begin{bmatrix}
x_1x_1 \\
x_1x_2 \\
x_2x_1 \\
x_2x_2 \\
\sqrt{2c}x_1 \\
\sqrt{2c}x_2 \\
\end{bmatrix}
\end{align}

<hr />

<ul>
<li>
Given \(x, z \in \mathbb{R}^n\)

<ul>
<li>
\(K(x, z) = (x^Tz+ c)^d\)

<li>
Where \(\phi(x)\) contains the \(\binom{n+d}{d}\) combinations of monomials of degree d. (Note: a monomial of degree 3 could be \(x_1x_2x_3\) or \(x_1x_2^2\), etc)

</ul>
</ul>
</div>
  </body>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/highlight.min.js"
  ></script>
  <script
    type="text/javascript"
    src="https://albamr09.github.io/zepto.min.js"
  ></script>
  <script type="text/javascript">
    $("pre").each(function (index, item) {
      $(item).html("<code>" + $(item).html() + "</code>");
    });
    hljs.initHighlightingOnLoad();
  </script>
</html>
